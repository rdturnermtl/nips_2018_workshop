\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{booktabs}
\usepackage{siunitx}

\title{Formatting instructions for NIPS 2018}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\newcommand{\code}{\texttt}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% Motivation
Rigourous design of machine learning competitions is at least as difficult as rigrouous design of experiments.
It is an experiment across comparing multiple machine learning algorithms (possibly with adversarial agents)\@.
Part of experimental design involves proper analysis with confidence intervals (error bars) and statistical tests.

% TODO look for citation show frequency
Surprisingly adding error bars and significance levels has been rarer in machine learning than other parts of science.
This is somewhat surprising given that machine learning methodology is often based upon statistics.
Many comptetitions and benchmark data sets follow a plateau like ``leader board'' with big gains at first followed by a long period of very small gains.
Once these gains become very small and incremental it is easy for these gains to be explainable purely by noise.

One of the obstacles inhibiting widespread use of these statistical methods is tools that introduce minimal time overhead in use.
Indeed many other areas have tools that standardize the analysis.  % TODO reword
Benchmark tools is designed to modular.
However, it can be called in a simple all one call fasion using a dictionary of sklearn compatible objects and a train and test data set.
The routine will train, test, and benchmark the models on multiple loss functions.

Considering multiple loss functions is a crucuial part of the package.
Often challenges and analysis are based upon a single somewhat arbitrary loss function; and developers may become obsessed with incremental improvements in a single metric.
Therefore, we allow a single model to be benchmarked according to multiple metrics.
However, a crucial piece of this is that the benchmark tools package supports the Bayes' decision rule calculation that can convert a predictive distribution into an action.
Indeed the classic example for regression is for MSE one should report the mean of the predictive distribution while for MAE one should report the median.
This conversion is done automatically within the package and is essential if one wants to ensure a single model is being benchmarked fairly and consistently across multiple metrics.

\section{The interface}

% TODO accro
Benchmark tools was largely motivated by noticing that on any applied ML project there many repeated ``boilerplate'' code segments that frequently repeat across projects.
These based upon splitting the data, training the models, testing the models, and doing a benchmark analysis with statistical analysis.
Also, proper formatting of tables for presentation of the results is also a common repetetive task.

The high level interface of the package has just two phases: \code{just\_benchmark} and \code{just\_format\_it}.
These work as follows:	
\begin{verbatim}
import benchmark_tools.classification as btc
from benchmark_tools.classification import STD_CLASS_LOSS, STD_BINARY_CURVES
performance_df, performance_curves_dict = \
    btc.just_benchmark(X_train, y_train, X_test, y_test, 2, classifiers,
                       STD_CLASS_LOSS, STD_BINARY_CURVES, ref_method)
\end{verbatim}
This benchmarks all the models in the classifiers dictionary on the data \code{(X\_train, y\_train, X\_test, y\_test)} for 2-class classification.
It uses the loss function described in the dictionaries \code{STD\_CLASS\_LOSS}, and the curves (e.g., ROC, PR) in \code{STD\_BINARY\_CURVES}.
\code{ref\_method} defines the model that is the reference to compare against (i.e., for a paired statistical test giving tighter error bars) for assessing statistically significant performance gains.
The \code{classifiers} dictionary is as simple as:
\begin{verbatim}
classifiers = \
    {'Nearest Neighbors': KNeighborsClassifier(3),
     'Linear SVM': SVC(kernel='linear', C=0.025, probability=True),
     'RBF SVM': SVC(gamma=2, C=1, probability=True),
     ...
\end{verbatim}
The objects need not be sklearn objects but merely support the methods \code{fit} and \code{predict\_log\_proba} as per the sklearn interface.
Equivalent routines are available in \code{benchmark\_tools.regression} for regression problems.

Although the most convenient way to benchmark is via the ``do-it-all`` \code{just\_benchmark} routine, the package also allows modular access.
The \code{just\_benchmark} routine works via xxx phases.
These are (with their corresponding routine): get_pred_log_prob, loss_table, loss_summary_table.

The \code{performance\_df} is a pandas dataframe with a table summarizing the performance.
However, for publishable results one must first format it correctly:
The sciprint module formats these tables for scientific presentation.
The performance dictionaries can be converted to cleanly formatted tables: correct significant figures, shifting of exponent for compactness, thresholding huge/small (crap limit) results, and correct alignment of decimal points, units in headers, etc.
Here we use:
\begin{verbatim}
import benchmark_tools.sciprint as sp
print sp.just_format_it(performance_df, shift_mod=3, unit_dict={'NLL': 'nats'},
                        crap_limit_min={'AUPRG': -1},
                        EB_limit={'AUPRG': -1},
                        non_finite_fmt={sp.NAN_STR: '{--}'}, use_tex=True)
\end{verbatim}
Both plain text and \LaTeX tables are available via the \code{use\_tex} argument.
The above snippet produces the \LaTeX table:\\
{\tiny
\begin{tabular}{|l|Sr|Sr|Sr|Sr|Sr|Sr|Sr|}
\toprule
{}                &       {AP} &      {p} &      {AUC} &      {p} &  {AUPRG} &      {p} &    {Brier} &      {p} & {NLL (nats)} &      {p} &   {sphere} &      {p} & {zero one} &      {p} \\
\midrule
AdaBoost          &  0.93(16)  &  <0.0001 &  0.950(96) &  <0.0001 &  0.90464 &  <0.0001 &  0.42(14)  &  <0.0001 &    0.368(80) &  <0.0001 &  0.36(15)  &  <0.0001 &  0.075(86) &  <0.0001 \\
Decision Tree     &  0.95(13)  &  <0.0001 &  0.966(70) &  <0.0001 &  0.93860 &  <0.0001 &  0.18(25)  &  <0.0001 &    0.40(71)  &   0.4072 &  0.16(22)  &  <0.0001 &  0.050(71) &  <0.0001 \\
Gaussian Process  &  0.90(22)  &  <0.0001 &  0.95(12)  &  <0.0001 &  0.92081 &  <0.0001 &  0.27(17)  &  <0.0001 &    0.27(11)  &  <0.0001 &  0.22(16)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Linear SVM        &  0.952(99) &  <0.0001 &  0.950(77) &  <0.0001 &  0.88705 &  <0.0001 &  0.34(24)  &  <0.0001 &    0.29(16)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.15(12)  &   0.0006 \\
Naive Bayes       &  0.957(97) &  <0.0001 &  0.957(68) &  <0.0001 &  0.89782 &  <0.0001 &  0.34(25)  &  <0.0001 &    0.28(18)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.13(11)  &   0.0002 \\
Nearest Neighbors &  0.94(14)  &  <0.0001 &  0.969(69) &  <0.0001 &  0.93498 &  <0.0001 &  0.18(21)  &  <0.0001 &    0.42(70)  &   0.4241 &  0.15(18)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Neural Net        &  0.957(91) &  <0.0001 &  0.957(69) &  <0.0001 &  0.89782 &  <0.0001 &  0.33(23)  &  <0.0001 &    0.28(15)  &  <0.0001 &  0.30(22)  &  <0.0001 &  0.100(98) &  <0.0001 \\
QDA               &  0.951(91) &  <0.0001 &  0.950(80) &  <0.0001 &  0.88517 &  <0.0001 &  0.34(27)  &  <0.0001 &    0.29(21)  &   0.0003 &  0.31(25)  &  <0.0001 &  0.15(12)  &   0.0006 \\
RBF SVM           &  0.93(18)  &  <0.0001 &  0.957(94) &  <0.0001 &  0.92081 &  <0.0001 &  0.14(20)  &  <0.0001 &    0.18(18)  &  <0.0001 &  0.12(17)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Random Forest     &  0.965(82) &  <0.0001 &  0.949(84) &  <0.0001 &  0.92147 &  <0.0001 &  0.31(26)  &  <0.0001 &    0.52(70)  &   0.6099 &  0.28(24)  &  <0.0001 &  0.100(98) &  <0.0001 \\
iid               &  0.53(16)  &     {--} &  0.5(0)    &     {--} &  0(0)    &     {--} &  1.004(22) &     {--} &    0.695(11) &     {--} &  1.005(27) &     {--} &  0.53(17)  &     {--} \\
\bottomrule
\end{tabular}
}

\section{Loss functions}

Benchmark tools is based upon two types of metrics: \emph{loss functions} and \emph{curve summaries}.
From a decision theoretic perspective loss functions are the more justified metric for evaluation; they are also easier to use place confidence intervals on.
The loss for model $A$ on iid test data with labels $y_{1:N}$ works as:
\begin{align}
  L_A = \sum_{i=1}^N \ell_i = \sum_{i=1}^N \ell(y_i, a_i)\,, \quad a_i = \argmin_a \E_{P_A(y_i|x_i)}[\ell(y_i, a)]\,,
\end{align}
where $a_i$ is the Bayes' optimal action for the loss function $\ell$.
Benchmark tools has built in support for general \emph{loss matrices} for hard classification, but also also supports the log loss (NLL), Brier score, and sphereical score as \emph{proper scoring rules} to evaluate a models soft predictions.
Non-probalistic methods are still usable within this framework by ``pipelining'' with a \emph{calibrator}, some of which are already available in sklearn.

Curve summaries are based upon creating a performance curve and then summarizing with a single number.
The supported performance curves are ROC, precision-recall, and precision-recall-gain.
We currently only use the summary of area under the curve.
Great care is taken to ensure these curves and summaries and unbiased and behave correctly for a random classifier, something that was \emph{not} done in sklearn.  % TODO ref

\section{Error bars}

Putting error bars on the loss functions are essentially placing an error bar on the mean.
Note this places a confidence interval on what the performance would be on a new ($N \rightarrow \infty$) test set from the same distribution, and with the same trained model.
Given the individual losses $\ell_{1:N}$, we support three methods for confidence intervals: t-test, bootstrap, and bernstein bounds.
The t-test is fairly standard, but makes a central limit assumption that the error distribution on the mean is normally distributed.
We also use the percentile boostrap to construct the error bars, which still makes some assymptotic assumptions but weaker than the t-test.
For very conservative error bars, one can use the Bernstein bounds, which is distribution free and holds for finite sample.  % TODO cite
However, these error bars requires a \emph{bounded loss}, otherwise it result in infinite error bars.

For tighter error bars we support the option \code{pairwise\_CI}, which build the error bars from $\ell_A - \ell_R$ to compare models $A$ to the reference model $R$.
Since, the errors in two models are often correlated, the pairwise errorbars with be tighter.
The p-value column always use a paired test to the reference model to construct the p-values.

The error bars and significance tests for the curve summaries are produced via bootstrap.
We a vectorized bootstrap that reweights the data point via a multinomial distribution.
This avoids re-creating the data sets in memory upon resampling, which is very slow for large data sets.
For example, the MATLAB bootstrap routine takes this approach, but can often be slower than training the classifier itself for large data sets.  % TODO ref

\section{conclusions}

% A few sentences on future work




% Less repetetive, easy to add CIs, stat sig
% Why useful for challenges

% Workflow: from sklean dict
% Figure

% Tasks: classificatio and reg

% How math in each works
% proper scoring rules
% error bars, and how stat matches

% A note on unit tests

% sciprint
% principles on correct formatting

% Can copy over examples from README

% refs: proper scoring, AUPRG, bernstein
% sklearn bug

% include URL

\end{document}
