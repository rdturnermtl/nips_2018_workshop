\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage[nonatbib,final]{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[numbers]{natbib}
\usepackage{booktabs}
\usepackage{siunitx}

\title{The Machine Learning Benchmark Tools Package}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Ryan Turner \\
  Uber AI Labs
}

\renewcommand{\vec}[1]{{\boldsymbol{\mathbf{#1}}}} % vector
\newcommand{\mat}[1]{{\ensuremath{\mathbf{#1}}}} % matrix

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\loss}{\ell}
\newcommand{\sample}{\sim}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\code}{\texttt}

% TODO
% package name
% compress

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\vspace{-5mm}

% Motivation
Rigorous design of machine learning (ML) challenges is at least as difficult as rigorous design of experiments.
Challenges are experiments comparing multiple machine learning algorithms (possibly with adversarial agents)\@.
Part of experimental design involves proper analysis with confidence intervals (error bars) and statistical tests.

Surprisingly, error bars and significance levels are rarer in ML than other parts of science, even though ML methodology is often based upon statistics.
Challenges and benchmark data sets tend to plateau: Initial big gains are followed by a long period of incremental gains.
Once these gains become small, they are often explainable purely by sampling noise.

To enable widespread error analysis with minimal time overhead we present the \emph{Benchmark Tools} Python package.\footnote{This package is found at
\url{https://github.com/rdturnermtl/benchmark_tools}.}
Benchmark Tools is usable in a simple one-liner fashion using a dictionary of sklearn compatible objects~\citep{Pedregosa2011}, along with a train and test data set.
The routine will train, test, and evaluate the models on multiple loss functions.
Individual pieces of this process are usable in a modular way.

Considering multiple loss functions is a design principle of Benchmark Tools.
Often challenges and analysis are based upon a single (somewhat arbitrary) loss function, and ML developers often become obsessed with incremental improvements in a single metric.
The Benchmark Tools package supports the Bayes' decision rule calculation that converts a predictive distribution into a different \emph{action} for each loss function.
The classic example for regression is for MSE one should report the mean of the predictive distribution while for MAE one should report the median~\citep{Marchini2013}.
This conversion is done automatically within the package and is essential if one wants to ensure a single model is being benchmarked fairly and consistently across multiple metrics.
The classification benchmark can build loss functions with arbitrary loss matrices; this allows, for example, probabilistic predictions to be converted to a ``don't know'' option and evaluated.

\section{The interface}

Benchmark Tools was largely motivated by noticing that applied ML projects contain many repeated ``boilerplate'' code segments that frequently repeat across projects.
These typically consist of: splitting data, training models, testing models, evaluating with statistical analysis, and finally proper formatting of tables.

The high level interface of the package has just two phases: \code{just\_benchmark} and \code{just\_format\_it}.
These work as follows:
\vspace{-3mm}
\begin{verbatim}
import benchmark_tools.classification as btc
from benchmark_tools.classification import STD_CLASS_LOSS, STD_BINARY_CURVES
performance_df, performance_curves_dict = \
    btc.just_benchmark(X_train, y_train, X_test, y_test, 2, classifiers,
                       STD_CLASS_LOSS, STD_BINARY_CURVES, ref_method='iid')
\end{verbatim}
\vspace{-3mm}
This benchmarks all the models in the classifiers dictionary \code{classifiers} on the data \code{(X\_train, y\_train, X\_test, y\_test)} for 2-class classification.
It uses the loss function described in the dictionaries \code{STD\_CLASS\_LOSS}, and the curves (e.g., ROC, PR) in \code{STD\_BINARY\_CURVES}.
\code{ref\_method} defines the model that is the reference to compare against (using a paired statistical test giving tighter error bars) for assessing statistically significant performance gains.
We provide an iid dummy model (\code{JustNoise}) that provides constant predictions to serve as a simple baseline.
The \code{classifiers} dictionary is as simple as:
\vspace{-3mm}
\begin{verbatim}
classifiers = {'iid': btc.JustNoise(),
               'Nearest Neighbors': KNeighborsClassifier(3),
               'Linear SVM': SVC(kernel='linear', C=0.025, probability=True),
               'RBF SVM': SVC(gamma=2, C=1, probability=True),
               ...
\end{verbatim}
\vspace{-3mm}
The objects need not be sklearn objects but merely support the methods \code{fit} and \code{predict\_log\_proba} as per the sklearn interface.
Equivalent routines are available in \code{benchmark\_tools.regression} for regression problems.

Although the most convenient way to benchmark is via the ``do-it-all'' \code{just\_benchmark} routine, the package also allows modular usage.
The \code{just\_benchmark} routine works via three phases.
These are (with their corresponding routine) to build dataframe of: predictive distributions on each test point and model \code{get\_pred\_log\_prob}, the losses for each prediction \code{loss\_table}, mean loss for each method along with error bars and p-values \code{loss\_summary\_table}.

The \code{performance\_df} is a pandas dataframe with a table summarizing the performance.
However, for publishable results one must first format it correctly:
The \emph{sciprint} module formats these tables for scientific presentation~\citep{Cole2015}.
The performance dataframe is converted to cleanly formatted tables: correct significant figures, shifting of exponent for compactness, and correct alignment of decimal points, units in headers, etc.
Here we use:
\vspace{-3mm}
\begin{verbatim}
import benchmark_tools.sciprint as sp
print sp.just_format_it(performance_df, shift_mod=3, unit_dict={'NLL': 'nats'},
                        non_finite_fmt={sp.NAN_STR: '{--}'}, use_tex=True)
\end{verbatim}
\vspace{-3mm}
Both plain text and \LaTeX\ tables are available via the \code{use\_tex} argument.
The above snippet produces the \LaTeX\ table:
\begin{center}
{\tiny
\setlength{\tabcolsep}{0.75em} % for the horizontal padding
\begin{tabular}{|l|l|r|l|r|l|r|l|r|l|r|l|r|l|r|}
\toprule
{}                &       {AP} &      {p} &      {AUC} &      {p} &  {AUPRG} &      {p} &    {Brier} &      {p} & {NLL (nats)} &      {p} &   {sphere} &      {p} & {zero one} &      {p} \\
\midrule
AdaBoost          &  0.93(16)  &  <0.0001 &  0.950(96) &  <0.0001 &  0.90464 &  <0.0001 &  0.42(14)  &  <0.0001 &    0.368(80) &  <0.0001 &  0.36(15)  &  <0.0001 &  0.075(86) &  <0.0001 \\
Decision Tree     &  0.95(13)  &  <0.0001 &  0.966(70) &  <0.0001 &  0.93860 &  <0.0001 &  0.18(25)  &  <0.0001 &    0.40(71)  &   0.4072 &  0.16(22)  &  <0.0001 &  0.050(71) &  <0.0001 \\
Gaussian Process  &  0.90(22)  &  <0.0001 &  0.95(12)  &  <0.0001 &  0.92081 &  <0.0001 &  0.27(17)  &  <0.0001 &    0.27(11)  &  <0.0001 &  0.22(16)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Linear SVM        &  0.952(99) &  <0.0001 &  0.950(77) &  <0.0001 &  0.88705 &  <0.0001 &  0.34(24)  &  <0.0001 &    0.29(16)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.15(12)  &   0.0006 \\
Naive Bayes       &  0.957(97) &  <0.0001 &  0.957(68) &  <0.0001 &  0.89782 &  <0.0001 &  0.34(25)  &  <0.0001 &    0.28(18)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.13(11)  &   0.0002 \\
Nearest Neighbors &  0.94(14)  &  <0.0001 &  0.969(69) &  <0.0001 &  0.93498 &  <0.0001 &  0.18(21)  &  <0.0001 &    0.42(70)  &   0.4241 &  0.15(18)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Neural Net        &  0.957(91) &  <0.0001 &  0.957(69) &  <0.0001 &  0.89782 &  <0.0001 &  0.33(23)  &  <0.0001 &    0.28(15)  &  <0.0001 &  0.30(22)  &  <0.0001 &  0.100(98) &  <0.0001 \\
QDA               &  0.951(91) &  <0.0001 &  0.950(80) &  <0.0001 &  0.88517 &  <0.0001 &  0.34(27)  &  <0.0001 &    0.29(21)  &   0.0003 &  0.31(25)  &  <0.0001 &  0.15(12)  &   0.0006 \\
RBF SVM           &  0.93(18)  &  <0.0001 &  0.957(94) &  <0.0001 &  0.92081 &  <0.0001 &  0.14(20)  &  <0.0001 &    0.18(18)  &  <0.0001 &  0.12(17)  &  <0.0001 &  0.025(51) &  <0.0001 \\
Random Forest     &  0.965(82) &  <0.0001 &  0.949(84) &  <0.0001 &  0.92147 &  <0.0001 &  0.31(26)  &  <0.0001 &    0.52(70)  &   0.6099 &  0.28(24)  &  <0.0001 &  0.100(98) &  <0.0001 \\
iid               &  0.53(16)  &     {--} &  0.5(0)    &     {--} &  0(0)    &     {--} &  1.004(22) &     {--} &    0.695(11) &     {--} &  1.005(27) &     {--} &  0.53(17)  &     {--} \\
\bottomrule
\end{tabular}
}
\end{center}
The sciprint module automatically enforces good practices for presentation of numeric results.
Sciprint crops the significant figures on the performance number to match the specified number of error digits (with a default of 2 seen above), but values of 1 or 2 are typical.
The error digits are provided in parentheses.
The p-values are limited to specified number of digits (with a default of 4), but values in the range 2--4 are typical.
The values are correctly aligned at the decimal point.
To aid in this, the package automatically adjusts the exponent of each column (use \code{shift\_mod=3} to ensure a modulus of 3 to enable SI prefixes) to minimize its width for written compactness.
For this reason, metrics are best used as columns with models as rows, because the scale/units are fixed across metrics.

The package also comes with a \emph{data splitter} module that supports random, ordinal, or temporal splitting across features in pandas dataframes.
It also allows for jointly splitting across multiple features to test difficult generalization cases (e.g., test set of random unseen users \emph{and} later in time than training)\@.

\section{Loss functions}

Benchmark Tools is based upon two types of metrics: \emph{loss functions} and \emph{curve summaries}.
From a decision theoretic perspective loss functions are the more justified metric for evaluation; they are also easier to place confidence intervals on.
The loss for model $A$ on iid test data with labels $y_{1:N}$ works as:
\begin{align}
  \Loss_A = \sum_{i=1}^N \loss_i = \sum_{i=1}^N \loss(y_i, a_i)\,, \quad a_i = \argmin_a \E_{P_A(y_i|\vec x_i)}[\loss(y_i, a)]\,,
\end{align}
where $a_i$ is the Bayes' optimal action for the loss function $\loss$.
Benchmark Tools has built-in support for general \emph{loss matrices} for hard classification, but also also supports the log loss (NLL), Brier loss, and spherical loss as \emph{proper scoring rules}~\citep{Gneiting2007} to evaluate a models' soft predictions.
The modular system is extendable allowing for new metrics to be easily added.
Non-probabilistic methods are still usable within this framework by ``pipelining'' with a \emph{calibrator}~\citep{Kull2017,Platt1999}, some of which are already available in sklearn.

Curve summaries are based upon creating a performance curve and then summarizing with a single number.
The supported performance curves are ROC, precision-recall, and precision-recall-gain~\citep{Flach2015}.
We currently only use the summary of area under the curve (AUC)\@.
Great care is taken to ensure these curves and summaries are unbiased and behave correctly for a random classifier, something that was \emph{not} done in sklearn (e.g., see issue \#4577)\@.

\section{Error bars}

Putting error bars on the loss functions are essentially placing an error bar on the mean of $\loss$.
Note this places a confidence interval on what the performance would be on a new ($N \rightarrow \infty$) test set from the same distribution, and with the same trained model.
Given the individual losses $\loss_{1:N}$, we support three methods for confidence intervals: t-test, bootstrap, and Bernstein bound.
The t-test is fairly standard, but makes a central limit assumption that the error distribution on the mean is normally distributed.
We also use the percentile bootstrap to construct the error bars, which still makes some asymptotic assumptions but weaker than the t-test.
For very conservative error bars, we offer the Bernstein bound, which is distribution free and holds for finite sample~\citep{Audibert2009}.
However, these error bars requires a \emph{bounded loss}, otherwise it results in infinite error bars.

For tighter error bars we support the option \code{pairwise\_CI}, which builds the error bars from $\loss_A - \loss_R$ to compare model $A$ to the reference model $R$.
Since, the errors in two models are often correlated, the pairwise error bars will be tighter.
The p-value column always uses a paired test to the reference model $R$ to construct the p-values.
% Would be good to put in CI eqns but just no space

The p-values are designed to match the error bars, and therefore constructed using either t-test, bootstrap, or Bernstein bound.
The confidence interval $(U,L)$ on $\Delta \Loss := \Loss_A - \Loss_R$ and p-value $p$ (on $H_0$:~$\Loss_A = \Loss_R$) are designed to be coherent in the sense that: $L = 0$ or $U = 0$ if the confidence interval is computed with coverage $\alpha=1-p$.
Construction of p-values from the t-test and bootstrap are standard, but Bernstein is more novel.
% Would be good to use p-val eqns but just no space

The error bars and significance tests for the curve summaries are produced via bootstrap~\citep{Efron1994}.
We built a vectorized bootstrap that reweights the data points via a multinomial distribution.
This avoids re-creating the data sets in memory upon resampling, which is very slow for large data sets.
For example, the MATLAB bootstrap routine (\code{bootstrp}) takes this approach, and is often slower than training the classifier itself for large data sets.

\section{Conclusions}

We have presented a package with utility to be used in many ML challenges and publications.
The use of one-liner performance tables with error bars and significance levels should make its use universal in ML\@.
The package still has much potential for expansion beyond classification and regression to more complex tasks such as structured prediction problems.

\paragraph{Acknowledgments}
We thank Riashat Islam for setting up continuous integration testing in the Benchmark Tools package.

\bibliographystyle{abbrvnat}
\bibliography{btpaper} % References file

\end{document}

